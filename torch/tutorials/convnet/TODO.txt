* Move filter-classification code out of bin/classify-filters.lua and
  into a module in convnet/.  I'll need to do more coding to support
  the model phenomenology and error analysis stage of the research.
  There is, for example, more than one way to measure the polarity
  (positive or negative) of an n-gram.

    1) Counting the positive or negative filters triggered.  Related to
       this is highlighting the positions in the sentence that triggered
       no filters of any kind.
    2) Performing a sensitivity analysis or ablation study; quantifying
       the change in the model's output when the output of filters
       triggered by an n-gram are zeroed.  

* Given a sentence, a model, and a vector identifying the model's positive
  and negative filters, output the model's prediction, its softmax output,
  and -- for each position in a sentence -- both the number of positive
  or negative filters it triggered and its contribution to a positive
  or negative prediction.

* Implement a builder that takes a trained temporal convolutional model
  and builds a model that has weight updates disabled for all layers
  and that has in its first layer a sentence matrix (possibly random,
  possibly consisting of vectors from a trained lookup table).  The
  purpose of this builder is to 

* Final models:
    * Wikipedia
        * Collobert & Weston model with no hidden layers.
            * results/wiki/collobert/1m/run.sh
        * Okanohara model with no hidden layers.
            * results/wiki/okanohara/1m/run.sh
        * Permutation model with no hidden layers.
            * results/wiki/permutation/1m/run.sh
    * BNC
        * Collobert & Weston model with no hidden layers.
            * results/test-bnc/collobert/400k/run.sh
        * Okanohara model with no hidden layers.
            * results/test-bnc/okanohara/400k/run.sh
        * Permutation model with no hidden layers.
            * results/test-bnc/permutation/400k/run.sh

* Data sets:
    * BNC
    * Wikipedia (same size as BNC)
    * Wikipedia (much larger)
    * Sentences from ASAP short answer data set
    * Sentences from ETS grammaticality data set

* Models
    * Temporal convolutional network
    * Spatial convolutional network
    * Dynamic spatial convolutional network (DCNN, Kalchbrenner)
    * RNN language model (co-trained with logistic layer?)

* Error analysis:
    * Port index.pkl file from Python to Torch.
    * Write function to convert sentence to a vector, run it
      through a trained network, and return the prediction 
      and fraction of positive/negative activations at each
      word.
    * Get test set predictions from a model.
    * Get indices of activations for each sentence in test set.
    * Classify features as either positive or negative.
    * Here the workflow changes to Python:
        * For each sentence 
            * Get outputs/indices of activations from positive filters.
            * Then, for each position in the sentence, sum the outputs
              of the positive filters.  (If a filter is activated at
              position i, and the filter is of width k, the output 
              of the filter is copied to the words at positions i,
              i+1, ..., i+k-1.)
            * Do the same for the negative filters.
    * Compare log probabilities that our model assigns to sentences to
      to log probabilities assigned by a language model.  The hope is
      that our model is weakly -- or at least not strongly -- correlated
      with a language model.

* Curriculum learning
    * Try training first using only long positive and negative examples.
    * After each epoch, evaluate the model separately on short examples
      and long examples.
    * If performance improves nicely, gradually introduce short examples
      into the training set.

* Model enhancements:
  * I'd like the output of a filter to be a richer description of the
    configuration of the sequence of word vectors.  In a temporal 
    convolutional network, applying a filter to a sentence outputs a
    scalar value for each position in the sentence.   If the discrete
    convolution were replaced by a dot product at each position in the
    sentence, the output would be a kXk matrix for each position (where
    k is the temporal width of the filter).  The ij-th entry of those
    matrices would contain the dot product of the i-th filter row and
    the j-th word vector of an input sequence.
    
    * Create layer to shatter the sentence matrix created by a LookupTable
      into an nFilters X filterWidth X wordDim 3-tensor.
    * Copy and modify nn.MM so that it has its own 3-tensor of weights
      (of size nFilters X wordDim X filterWidth) and that it accumulates
      the gradient.

* Create layer to record summary statistics about a layer's
  post-activation outputs.  This can help diagnose problems
  with weight initialization -- particularly for deeper
  networks.

* I would like to be able to use TemporalConvolutionFB, but I get
  an error about tensor size when I do.  Put some print statements
  into the C code to isolate the problem, fix the code, and submit
  a pull request for fbcunn.  Note: this appears to be caused by
  a faulty assumption about the size of outputTH in one of the
  methods in fbcunn/src/ConvolutionBias.cu.  I opened this issue
  against the problem:
    https://github.com/facebook/fbcunn/issues/62

* I want to be able keep the word representations on the surface of
  the L2 ball (i.e. to renorm them to L2 length 1 after every minibatch),
  but the renorm call is failing with a CUDA error.  I haven't been able
  to reproduce the error using a minimal working example.  My workaround
  is to copy the weights, renormalize them, then assign them back to
  the lookup table.

== DONE ==

* Create renorming module to enable easy selective renorming of layers.

    model = nn.Sequential()
    layer = nn.Linear(10, 5)
    model:add(layer)

    -- e.g.
    renormer = kttorch.Renormer()
    renormer:add(layer, 10)
    -- or
    renormer:add(layer, 10, { p=2, dim=1 })
    -- then
    renormer:renorm()

* Create dropout layers intended to appear prior to a LookupTable.  The
  layers are intended to drop words from the input.  Ways of dropping
  words from the input include but are not limited to:
    * Deleting elements from the input with some fixed probability.
    * Setting elements of the input to the index of the zero vector
      (in the weight matrix of the downstream LookupTable) with
     some fixed probability.
  Another possibility is to allow the dropout probability to vary
  based on some characteristic of the training data (e.g. word
  frequencies).

* Re-add Dropout(0.5) for the temporal pooling layer.

* Create a FixedLookupTable class that delegates either to
  LookupTable or LookupTableGPU but has an empty parameter
  accumulation method, so the word representations aren't
  parameters of the model.

* Try training with word2vec and FixedLookupTable, on the assumption
  that doing so will reduce the number of trainable parameters of
  the model and thereby reduce overfitting.

* Refactor the test funcstion in 5_test.lua to take a data set as
  an argument.  This will make it possible and easy for the validation
  and test sets both to be optional.

* Change 1_data.lua to pull a validation set of some user-specified
  number of sentences out of the training set.  This behavior should
  be optional and off by default.  The training data is assumed to
  be sorted randomly already, so select the validation set from the
  last n training examples.

* Use Renormer module in 4_train.lua.

* The zero vector appears to have an inordinate influence on training
  performance.   Re-setting it to zero after each weight update seems
  to trip up optimization process; performance goes from the initial
  random state to some improvement, then back down to random, where it
  seems to stay.

* Add function for randomly replacing a word in a sentence.  This should
  allow to start training on Wikipedia very soon.  The function should be
  called once for the validation and test sets before training commences,
  and at the beginning of each epoch for the training set.

* Train convnet in cosine space using torch.renorm for word
  representations and convnet/renorming.lfor components of
  kernels.  Use (the as-yet not complete):

    results/test-wiki/okanohara/1m-cosine-space/run.sh 

  The network appears to have no hope of converging using
  this approach.  The weights of the higher layers of the
  network hardly change.  Why does doing the convolution 
  in cosine space cause this behavior?

* Add makePermutationNegativeExamples to convnet/data.lua.

* Something seems broken with Collobert-style negative examples.  The
  model is not converging after I checked in changes to convnet/data.lua
  and bin/run.lua.  Find and fix.
